# Machine-Learning
A repositoty for machine learning.

## 1.Linear_Regression
#### 理解原理
线性回归是一种用于回归问题的最常见的方法。我们可以分别从一般形式以及极大似然方面对其进行理解。
对我个人而言，一般形式更有助于直观理解[多个因变量共同造成了某个自变量]的结果这一理论。而通过极大似然估计的推导，则可以知道为什么线性回归的代价函数是均方误差。因为最大化似然函数（求取使得某种结果出现的最大概率值）其实就可以变换为最小化真实值与预测值的均方误差。
#### 损失函数、代价函数、目标函数的定义与区别
其代价函数是真实值与预测值的均方误差。

损失函数(Loss Function)：度量单样本预测的错误程度，损失函数值越小，模型就越好。
代价函数(Cost Function)：度量全部样本集的平均误差。
目标函数(Object Function)：代价函数和正则化函数，最终要优化的函数。
#### 优化方法
优化方法有：（1）最小二乘估计法求模型的最优参数。（2）梯度下降法。（3）（拟）牛顿法。

本次利用sklearn.linear_model库中的LinearRegression模型用于回归预测。以便加深理解。

## 2.Naive_Bayes
### 理解原理
贝叶斯方法是一种最典型的生成模型，即根据先验概率和联合概率分布，求得造成某个果的某个因的后验概率的多少。另外一种典型的生成模型是隐马尔科夫模型。

后验概率: 在贝叶斯统计中，一个随机事件或者一个不确定事件的后验概率是在考虑和给出相关证据或数据后所得到的条件概率。同样，后验概率分布是一个未知量（视为随机变量）基于试验和调查后得到的概率分布。“后验”在本文中代表考虑了被测试事件的相关证据。
后验概率，就是在知道“果”之后，去推测“因”的概率，也就是说，如果已经知道瓜是好瓜，那么瓜的颜色是青绿的概率是多少。后验和先验的关系就需要运用贝叶斯决策理论来求解。

### 贝叶斯公式
P(c|x) = P(x,c)/P(x)= {P(c) *P(x|c)}/P(x)

最大似然估计是一种“模型已定，参数未知”的方法。

### 优缺点
优点

朴素贝叶斯模型有稳定的分类效率。
对小规模的数据表现很好，能处理多分类任务，适合增量式训练，尤其是数据量超出内存时，可以一批批的去增量训练。
对缺失数据不太敏感，算法也比较简单，常用于文本分类。
缺点:

理论上，朴素贝叶斯模型与其他分类方法相比具有最小的误差率。但是实际上并非总是如此，这是因为朴素贝叶斯模型给定输出类别的情况下,假设属性之间相互独立，这个假设在实际应用中往往是不成立的，在属性个数比较多或者属性之间相关性较大时，分类效果不好。而在属性相关性较小时，朴素贝叶斯性能最为良好。对于这一点，有半朴素贝叶斯之类的算法通过考虑部分关联性适度改进。
需要知道先验概率，且先验概率很多时候取决于假设，假设的模型可以有很多种，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳。
由于我们是通过先验和数据来决定后验的概率从而决定分类，所以分类决策存在一定的错误率。
对输入数据的表达形式很敏感。

